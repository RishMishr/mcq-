{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# connect your personal google drive to store the trained model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viGnnAVAVsX4",
        "outputId": "50f717d1-55eb-4365-8c72-12c5bc1825b4"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gDOT_rEf9yFI"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4q45GmGwVbT"
      },
      "source": [
        "# !pip install --quiet transformers==2.9.0\n",
        "# !pip install --quiet nltk==3.4.5\n",
        "# !pip install nltk\n",
        "# !pip install transformers\n",
        "# !pip install sentencepiece "
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R07mVhsI4w45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a434dd-b075-4a23-a4ad-ae1da2eccf5a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "sentence1 = \"There is a bug on the table\"\n",
        "sentence2 = \"The software has a bug\""
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLt3WH3X5gJp"
      },
      "source": [
        "# An example of a word with two different senses\n",
        "# original_word = \"bug\"\n",
        "\n",
        "# syns = wn.synsets(original_word,'n')\n",
        "\n",
        "# for syn in syns:\n",
        "#   print (syn, \": \",syn.definition(),\"\\n\" )"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsxh1ZjF5d-j"
      },
      "source": [
        "# Distractors from Wordnet\n",
        "def get_distractors_wordnet(syn,word):\n",
        "    distractors=[]\n",
        "    word= word.lower()\n",
        "    orig_word = word\n",
        "    if len(word.split())>0:\n",
        "        word = word.replace(\" \",\"_\")\n",
        "    hypernym = syn.hypernyms()\n",
        "    if len(hypernym) == 0: \n",
        "        return distractors\n",
        "    for item in hypernym[0].hyponyms():\n",
        "        name = item.lemmas()[0].name()\n",
        "        #print (\"name \",name, \" word\",orig_word)\n",
        "        if name == orig_word:\n",
        "            continue\n",
        "        name = name.replace(\"_\",\" \")\n",
        "        name = \" \".join(w.capitalize() for w in name.split())\n",
        "        if name is not None and name not in distractors:\n",
        "            distractors.append(name)\n",
        "    return distractors\n",
        "\n",
        "\n",
        "# synset_to_use = wn.synsets(original_word,'n')[0]\n",
        "# distractors_calculated = get_distractors_wordnet(synset_to_use,original_word)\n",
        "\n",
        "# print (\"\\noriginal word: \",original_word.capitalize())\n",
        "# print (distractors_calculated)\n",
        "\n",
        "\n",
        "# original_word = \"cat\"\n",
        "# synset_to_use = wn.synsets(original_word,'n')[1]\n",
        "# distractors_calculated = get_distractors_wordnet(synset_to_use,original_word)\n",
        "\n",
        "# print (\"\\noriginal word: \",original_word.capitalize())\n",
        "# print (distractors_calculated)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rELsk4JIMhJ3"
      },
      "source": [
        "Download pre-trained BERT WSD from [here](https://entuedu-my.sharepoint.com/:f:/g/personal/boonpeng001_e_ntu_edu_sg/EiWzblOyyOBDtuO3klUbXoAB3THFzke-2MLWguIXrDopWg?e=08umXD)\n",
        "\n",
        "Click the download button at the top left of the link to download a file named \"bert_base-augmented-batch_size=128-lr=2e-5-max_gloss=6.zip\"\n",
        "\n",
        "Place the zip file in your Google drive home folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNz0zFZzrXqN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f69df1-83c7-4a7e-dfaa-a4989d5edeb3"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "bert_wsd_pytorch = \"/content/gdrive/MyDrive/akaike/NLP/bert_base-augmented-batch_size=128-lr=2e-5-max_gloss=6.zip\"\n",
        "extract_directory = \"/content/gdrive/MyDrive/akaike/NLP\"\n",
        "\n",
        "extracted_folder = bert_wsd_pytorch.replace(\".zip\",\"\")\n",
        "\n",
        "#  If unzipped folder exists don't unzip again.\n",
        "if not os.path.isdir(extracted_folder):\n",
        "  with zipfile.ZipFile(bert_wsd_pytorch, 'r') as zip_ref:\n",
        "      zip_ref.extractall(extract_directory)\n",
        "else:\n",
        "  print (extracted_folder,\" is extracted already\")"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/akaike/NLP/bert_base-augmented-batch_size=128-lr=2e-5-max_gloss=6  is extracted already\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o3IOPGdhH3G"
      },
      "source": [
        "Download pre-trained BERT WSD from [here](https://entuedu-my.sharepoint.com/:f:/g/personal/boonpeng001_e_ntu_edu_sg/EiWzblOyyOBDtuO3klUbXoAB3THFzke-2MLWguIXrDopWg?e=08umXD)\n",
        "\n",
        "Click the download button at the top left of the link to download a file named \"bert_base-augmented-batch_size=128-lr=2e-5-max_gloss=6.zip\"\n",
        "\n",
        "Place the zip file in your Google drive home folder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGufDy9O_E4z",
        "outputId": "19505f2f-e26e-41ce-c44f-ab9ae0e30283"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnmszaP9zSpe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f935be-2512-417c-e3a5-2595fd2bc0a5"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "from transformers import BertModel, BertConfig, BertPreTrainedModel, BertTokenizer\n",
        "\n",
        "class BertWSD(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        self.ranking_linear = torch.nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "# def _forward(args, model, batch):\n",
        "#     batch = tuple(t.to(args.device) for t in batch)\n",
        "#     outputs = model.bert(input_ids=batch[0], attention_mask=batch[1], token_type_ids=batch[2])\n",
        "\n",
        "#     return model.dropout(outputs[1])\n",
        "    \n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_dir = \"/content/gdrive/MyDrive/akaike/NLP/bert_base-augmented-batch_size=128-lr=2e-5-max_gloss=6\"\n",
        "\n",
        "\n",
        "model = BertWSD.from_pretrained(model_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
        "# add new special token\n",
        "if '[TGT]' not in tokenizer.additional_special_tokens:\n",
        "    tokenizer.add_special_tokens({'additional_special_tokens': ['[TGT]']})\n",
        "    assert '[TGT]' in tokenizer.additional_special_tokens\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    \n",
        "model.to(DEVICE)\n",
        "model.eval()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/gdrive/MyDrive/akaike/NLP/bert_base-augmented-batch_size=128-lr=2e-5-max_gloss=6 were not used when initializing BertWSD: ['similarity_loss_factor', 'similarity_linear.weight', 'ranking_loss_factor', 'similarity_linear.bias']\n",
            "- This IS expected if you are initializing BertWSD from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertWSD from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertWSD(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30523, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (ranking_linear): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0bWxo4vFUfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bafafcc5-86ed-419d-f043-7e3dca196d0d"
      },
      "source": [
        "import csv\n",
        "import os\n",
        "from collections import namedtuple\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "GlossSelectionRecord = namedtuple(\"GlossSelectionRecord\", [\"guid\", \"sentence\", \"sense_keys\", \"glosses\", \"targets\"])\n",
        "BertInput = namedtuple(\"BertInput\", [\"input_ids\", \"input_mask\", \"segment_ids\", \"label_id\"])\n",
        "\n",
        "\n",
        "\n",
        "def _create_features_from_records(records, max_seq_length, tokenizer, cls_token_at_end=False, pad_on_left=False,\n",
        "                                  cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
        "                                  sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
        "                                  cls_token_segment_id=1, pad_token_segment_id=0,\n",
        "                                  mask_padding_with_zero=True, disable_progress_bar=False):\n",
        "    \"\"\" Convert records to list of features. Each feature is a list of sub-features where the first element is\n",
        "        always the feature created from context-gloss pair while the rest of the elements are features created from\n",
        "        context-example pairs (if available)\n",
        "        `cls_token_at_end` define the location of the CLS token:\n",
        "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
        "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
        "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    for record in tqdm(records, disable=disable_progress_bar):\n",
        "        tokens_a = tokenizer.tokenize(record.sentence)\n",
        "\n",
        "        sequences = [(gloss, 1 if i in record.targets else 0) for i, gloss in enumerate(record.glosses)]\n",
        "\n",
        "        pairs = []\n",
        "        for seq, label in sequences:\n",
        "            tokens_b = tokenizer.tokenize(seq)\n",
        "\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "\n",
        "            # The convention in BERT is:\n",
        "            # (a) For sequence pairs:\n",
        "            #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "            #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
        "            #\n",
        "            # Where \"type_ids\" are used to indicate whether this is the first\n",
        "            # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "            # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "            # embedding vector (and position vector). This is not *strictly* necessary\n",
        "            # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "            # it easier for the model to learn the concept of sequences.\n",
        "            #\n",
        "            # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "            # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "            # the entire model is fine-tuned.\n",
        "            tokens = tokens_a + [sep_token]\n",
        "            segment_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "            tokens += tokens_b + [sep_token]\n",
        "            segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
        "\n",
        "            if cls_token_at_end:\n",
        "                tokens = tokens + [cls_token]\n",
        "                segment_ids = segment_ids + [cls_token_segment_id]\n",
        "            else:\n",
        "                tokens = [cls_token] + tokens\n",
        "                segment_ids = [cls_token_segment_id] + segment_ids\n",
        "\n",
        "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "            # tokens are attended to.\n",
        "            input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "            # Zero-pad up to the sequence length.\n",
        "            padding_length = max_seq_length - len(input_ids)\n",
        "            if pad_on_left:\n",
        "                input_ids = ([pad_token] * padding_length) + input_ids\n",
        "                input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
        "                segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
        "            else:\n",
        "                input_ids = input_ids + ([pad_token] * padding_length)\n",
        "                input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "                segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "            assert len(input_ids) == max_seq_length\n",
        "            assert len(input_mask) == max_seq_length\n",
        "            assert len(segment_ids) == max_seq_length\n",
        "\n",
        "            pairs.append(\n",
        "                BertInput(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_id=label)\n",
        "            )\n",
        "\n",
        "        features.append(pairs)\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJSpZRuOF-52"
      },
      "source": [
        "import re\n",
        "import torch\n",
        "from tabulate import tabulate\n",
        "from torch.nn.functional import softmax\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer\n",
        "import time\n",
        "\n",
        "\n",
        "MAX_SEQ_LENGTH = 12\n",
        "\n",
        "def get_sense(sent):\n",
        "  re_result = re.search(r\"\\[TGT\\](.*)\\[TGT\\]\", sent)\n",
        "  if re_result is None:\n",
        "      print(\"\\nIncorrect input format. Please try again.\")\n",
        "\n",
        "  ambiguous_word = re_result.group(1).strip()\n",
        "\n",
        "  results = dict()\n",
        "\n",
        "  wn_pos = wn.NOUN\n",
        "  for i, synset in enumerate(set(wn.synsets(ambiguous_word, pos=wn_pos))):\n",
        "      results[synset] =  synset.definition()\n",
        "\n",
        "  if len(results) ==0:\n",
        "    return (None,None,ambiguous_word)\n",
        "\n",
        "  # print (results)\n",
        "  sense_keys=[]\n",
        "  definitions=[]\n",
        "  for sense_key, definition in results.items():\n",
        "      sense_keys.append(sense_key)\n",
        "      definitions.append(definition)\n",
        "\n",
        "\n",
        "  record = GlossSelectionRecord(\"test\", sent, sense_keys, definitions, [-1])\n",
        "\n",
        "  features = _create_features_from_records([record], MAX_SEQ_LENGTH, tokenizer,\n",
        "                                            cls_token=tokenizer.cls_token,\n",
        "                                            sep_token=tokenizer.sep_token,\n",
        "                                            cls_token_segment_id=1,\n",
        "                                            pad_token_segment_id=0,\n",
        "                                            disable_progress_bar=True)[0]\n",
        "\n",
        "  with torch.no_grad():\n",
        "      logits = torch.zeros(len(definitions), dtype=torch.double).to(DEVICE)\n",
        "      # for i, bert_input in tqdm(list(enumerate(features)), desc=\"Progress\"):\n",
        "      for i, bert_input in list(enumerate(features)):\n",
        "          logits[i] = model.ranking_linear(\n",
        "              model.bert(\n",
        "                  input_ids=torch.tensor(bert_input.input_ids, dtype=torch.long).unsqueeze(0).to(DEVICE),\n",
        "                  attention_mask=torch.tensor(bert_input.input_mask, dtype=torch.long).unsqueeze(0).to(DEVICE),\n",
        "                  token_type_ids=torch.tensor(bert_input.segment_ids, dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
        "              )[1]\n",
        "          )\n",
        "      scores = softmax(logits, dim=0)\n",
        "\n",
        "      preds = (sorted(zip(sense_keys, definitions, scores), key=lambda x: x[-1], reverse=True))\n",
        "\n",
        "\n",
        "  # print (preds)\n",
        "  sense = preds[0][0]\n",
        "  meaning = preds[0][1]\n",
        "  return (sense,meaning,ambiguous_word)\n",
        "\n",
        "\n",
        "# sentence1 = \"Srivatsan loves to watch **anime** during his free time\"\n",
        "\n",
        "\n",
        "# sentence_for_bert = sentence1.replace(\"**\",\" [TGT] \")\n",
        "# sentence_for_bert = \" \".join(sentence_for_bert.split())\n",
        "# sense,meaning,answer = get_sense(sentence_for_bert)\n",
        "\n",
        "# print (sentence1)\n",
        "# print (sense)\n",
        "# print (meaning)\n",
        "\n",
        "# sentence2 = \"Srivatsan is annoyed by the **people** in his room\"\n",
        "# sentence_for_bert = sentence2.replace(\"**\",\" [TGT] \")\n",
        "# sentence_for_bert = \" \".join(sentence_for_bert.split())\n",
        "# sense,meaning,answer = get_sense(sentence_for_bert)\n",
        "\n",
        "# print (\"\\n-------------------------------\")\n",
        "# print (sentence2)\n",
        "# print (sense)\n",
        "# print (meaning)\n",
        "\n"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration,T5Tokenizer"
      ],
      "metadata": {
        "id": "ymUznSUHmZWL"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "SDh7hgqaAVBV"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
        "\n",
        "question_model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
        "question_tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
      ],
      "metadata": {
        "id": "uxqZBs8Dpa-b"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyazTS9RJ46n"
      },
      "source": [
        "def get_question(sentence,answer):\n",
        "  text = \"context: {} answer: {} </s>\".format(sentence,answer)\n",
        "  print (text)\n",
        "  max_len = 256\n",
        "  encoding = question_tokenizer.encode_plus(text,max_length=max_len, pad_to_max_length=True, return_tensors=\"pt\")\n",
        "\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = question_model.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=5,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  max_length=200)\n",
        "\n",
        "\n",
        "  dec = [question_tokenizer.decode(ids) for ids in outs]\n",
        "\n",
        "\n",
        "  Question = dec[0].replace(\"question:\",\"\")\n",
        "  Question= Question.strip()\n",
        "  return Question\n",
        "\n",
        "\n",
        "# sentence1 = \"Srivatsan loves to watch **cricket** during his free time\"\n",
        "# sentence2 = \"Srivatsan is annoyed by a **cricket** in his room\"\n",
        "\n",
        "\n",
        "# answer = \"cricket\"\n",
        "\n",
        "# sentence_for_T5 = sentence1.replace(\"**\",\" \")\n",
        "# sentence_for_T5 = \" \".join(sentence_for_T5.split()) \n",
        "# ques = get_question(sentence_for_T5,answer)\n",
        "# print (ques)\n",
        "\n",
        "\n",
        "# print (\"\\n**************************************\\n\")\n",
        "# sentence_for_T5 = sentence2.replace(\"**\",\" \")\n",
        "# sentence_for_T5 = \" \".join(sentence_for_T5.split()) \n",
        "# ques = get_question(sentence_for_T5,answer)\n",
        "# print (ques)\n"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKbPKBjr-KTp"
      },
      "source": [
        "def getMCQs(sent):\n",
        "  sentence_for_bert = sent.replace(\"**\",\" [TGT] \")\n",
        "  sentence_for_bert = \" \".join(sentence_for_bert.split())\n",
        "  # try:\n",
        "  sense,meaning,answer = get_sense(sentence_for_bert)\n",
        "  if sense is not None:\n",
        "    distractors = get_distractors_wordnet(sense,answer)\n",
        "  else: \n",
        "    distractors = [\"Word not found in Wordnet. So unable to extract distractors.\"]\n",
        "  sentence_for_T5 = sent.replace(\"**\",\" \")\n",
        "  sentence_for_T5 = \" \".join(sentence_for_T5.split()) \n",
        "  ques = get_question(sentence_for_T5,answer)\n",
        "  return ques,answer,distractors,meaning\n",
        "\n",
        "\n",
        "\n",
        "# print (\"\\n\")\n",
        "# question,answer,distractors,meaning = getMCQs(sentence1)\n",
        "# print (question)\n",
        "# print (answer)\n",
        "# print (distractors)\n",
        "# print (meaning)\n",
        "\n",
        "# print (\"\\n\")\n",
        "# question,answer,distractors,meaning = getMCQs(sentence2)\n",
        "# print (question)\n",
        "# print (answer)\n",
        "# print (distractors)\n",
        "# print (meaning)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp0Sa7EVdqgf"
      },
      "source": [
        "# More examples\n",
        "\n",
        "# sentence = \"John went to river **bank** to cry\"\n",
        "# # sentence = \"John went to deposit money in the **bank**\"\n",
        "\n",
        "# # sentence = \"John bought a **mouse** for his computer.\"\n",
        "# # sentence = \"John saw a **mouse** under his bed.\"\n",
        "\n",
        "\n",
        "# print (\"\\n\")\n",
        "# question,answer,distractors,meaning = getMCQs(sentence)\n",
        "# print (question)\n",
        "# print (answer)\n",
        "# print (distractors)\n",
        "# print (meaning)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "k__Uvy608I_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d5380a2-f885-49ef-fe5e-68f7164cf57e"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "vrj2SDGKVITd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c9b97b-fed8-48b8-95dc-88325d0bfecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyPDF2\n",
        "import PyPDF2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert"
      ],
      "metadata": {
        "id": "p_GR5--sAIap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bfa0d4e-e302-45b8-9031-3c50ff99fc60"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keybert in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.10/dist-packages (from keybert) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.22.4)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from keybert) (13.3.4)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.14.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (3.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.29.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.15.1+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (23.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (16.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.13.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.1.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT"
      ],
      "metadata": {
        "id": "FAKEfpfMBA4Y"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genWordList(text):\n",
        "  text = text.lower()\n",
        "  kw_model = KeyBERT()\n",
        "  keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1,1), stop_words=None,\n",
        "                                use_mmr=True, diversity= 0.4, use_maxsum=False, top_n=5)\n",
        "  word_list = []\n",
        "  for i in keywords:\n",
        "    word_list.append(i[0])\n",
        "  \n",
        "  return(word_list)"
      ],
      "metadata": {
        "id": "5XK27U2XA75J"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yPq-Dy62GcRR"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "def select_sentence_with_word(paragraph, wordd):\n",
        "    # Split paragraph into sentences\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', paragraph)\n",
        "\n",
        "    # Filter sentences that contain the word\n",
        "    filtered_sentences = [sentence for sentence in sentences if wordd.lower() in sentence.lower()]\n",
        "\n",
        "    # Randomly select a sentence\n",
        "    selected_sentence = random.choice(filtered_sentences) if filtered_sentences else None\n",
        "\n",
        "    return selected_sentence\n"
      ],
      "metadata": {
        "id": "8f6NGWsrUDAx"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def decorate_occurrences(l, S):\n",
        "    S= S.lower()\n",
        "    S2 = S\n",
        "    final = []\n",
        "    FINAL = []\n",
        "    for word in l:\n",
        "      # nt = S.count(word)\n",
        "      # sp = random.randint(1, nt)\n",
        "      S2 = select_sentence_with_word(S, word )\n",
        "      S2 = S2.replace(word, f'**{word}**',1)\n",
        "      question,answer,distractors,meaning = getMCQs(S2)\n",
        "      final = [question, answer]\n",
        "      for i in distractors:\n",
        "        final.append(i)\n",
        "        if(len(final)==5):\n",
        "          break\n",
        "      final.append(meaning)\n",
        "      FINAL.append(final)\n",
        "      print (question)\n",
        "      print (answer)\n",
        "      print (distractors)\n",
        "      print (meaning)\n",
        "      S2 = S\n",
        "    return FINAL\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xz_v2objCOeD"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to call"
      ],
      "metadata": {
        "id": "FYYUDh04MkCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def get_mca_questions(context):\n",
        "  imp_words = genWordList(context)\n",
        "\n",
        "  mca_questions = decorate_occurrences(imp_words, context)\n",
        "  return mca_questions "
      ],
      "metadata": {
        "id": "QjpZKCbSI6l5"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        text = ''\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "# Provide the file paths of your PDFs\n",
        "pdf_files = ['/content/gdrive/MyDrive/akaike/NLP/chapter-2.pdf', '/content/gdrive/MyDrive/akaike/NLP/chapter-3.pdf', '/content/gdrive/MyDrive/akaike/NLP/chapter-4.pdf']\n",
        "\n",
        "# Create a text file to store the extracted text\n",
        "output_file = '/content/gdrive/MyDrive/akaike/NLP/all_output.txt'\n",
        "\n",
        "# Extract text from each PDF and store in the output file\n",
        "with open(output_file, 'w') as output:\n",
        "    for pdf_file in pdf_files:\n",
        "        text = extract_text_from_pdf(pdf_file)\n",
        "        output.write(text)\n",
        "\n",
        "print(\"Text extracted and saved to\", output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "091216ca-901b-4022-e55e-d6e1dfaf1277",
        "id": "9IxDv1I2LeAD"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text extracted and saved to /content/gdrive/MyDrive/akaike/NLP/all_output.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/gdrive/MyDrive/akaike/NLP/all_output.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "  text = file.read()\n",
        "\n",
        "text = text.lower()\n",
        "    "
      ],
      "metadata": {
        "id": "s4SBpcOVLeAG"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping the word 'mughals' as it is repeated too many times and in too many contexts. Also, it is not presesnt in wordnet."
      ],
      "metadata": {
        "id": "fd1pHIM6LzkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.replace('mughal', \"\")\n",
        "text = text.replace('mughals', \"\")\n",
        "text = text.replace('empire', \"\")"
      ],
      "metadata": {
        "id": "C-beCwX7LyKk"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = get_mca_questions(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaQ8aEBeLxU5",
        "outputId": "94c2076a-533d-445e-d6fa-8f6c6433e534"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context: after the death of aurangzeb, the bengal nawabs asserted their power and autonomy, as other regional powers were doing at that time. answer: nawabs </s>\n",
            "<pad>  Who asserted their power and autonomy after aurangzeb's death?</s>\n",
            "nawabs\n",
            "['Bey', 'Eparch', 'Governor General', 'Military Governor', 'Nawab', 'Proconsul', 'Satrap', 'Viceroy']\n",
            "a governor in India during the Mogul empire\n",
            "context: sometimes, the company forced the states into a “ subsidiary alliance”. answer: subsidiary </s>\n",
            "<pad>  What type of alliance does the company sometimes force the states into?</s>\n",
            "subsidiary\n",
            "['Broadcasting Company', 'Bureau De Change', 'Car Company', 'Closed Shop', 'Corporate Investor', 'Distributor', 'Dot-com', 'Drug Company', 'East India Company', 'Electronics Company', 'Film Company', 'Food Company', 'Furniture Company', 'Holding Company', 'Joint-stock Company', 'Limited Company', 'Livery Company', 'Mining Company', 'Mover', 'Oil Company', 'Open Shop', 'Packaging Company', 'Pipeline Company', 'Printing Concern', 'Record Company', 'Service', 'Shipper', 'Shipping Company', 'Steel Company', 'Stock Company', 'Subsidiary Company', 'Target Company', 'Think Tank', 'Transportation Company', 'Union Shop', 'White Knight']\n",
            "a company that is completely controlled by another company\n",
            "context: how did the powers of tribal chiefs change under colonial rule? answer: colonial </s>\n",
            "<pad>  Under what type of rule did tribal chiefs change?</s>\n",
            "colonial\n",
            "['Alexandrian', 'Coaster', 'Dalesman', 'Housemate', 'Inmate', 'Metropolitan', 'Outlier', 'Owner-occupier', 'Sojourner', 'Stater', 'Suburbanite', 'Tenant', 'Townsman']\n",
            "a resident of a colony\n",
            "context: they fought a prolonged war with afghanistan between 1838 and 1842 , and established indirect company rule there. answer: 1842 </s>\n",
            "<pad>  In what year did the British establish indirect company rule in Afghanistan?</s>\n",
            "1842\n",
            "['Word not found in Wordnet. So unable to extract distractors.']\n",
            "None\n",
            "context: the legend of tipu kings are often surrounded by legend and their powers glorified through folklore. answer: powers </s>\n",
            "<pad>  What are tipu kings glorified through folklore?</s>\n",
            "powers\n",
            "['Ability', 'Attitude', 'Cognitive Factor', 'Content', 'Episteme', 'Equivalent', 'History', 'Inability', 'Information', 'Lexis', 'Mind', 'Perception', 'Place', 'Practice', 'Process', 'Public Knowledge', 'Structure', 'Vocabulary']\n",
            "possession of the qualities (especially mental qualities) required to do something or get something done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnlc2TwzNaU3",
        "outputId": "1d85adac-426a-434e-b426-e72ed564014e"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[\"<pad>  Who asserted their power and autonomy after aurangzeb's death?</s>\",\n",
              "  'nawabs',\n",
              "  'Bey',\n",
              "  'Eparch',\n",
              "  'Governor General',\n",
              "  'a governor in India during the Mogul empire'],\n",
              " ['<pad>  What type of alliance does the company sometimes force the states into?</s>',\n",
              "  'subsidiary',\n",
              "  'Broadcasting Company',\n",
              "  'Bureau De Change',\n",
              "  'Car Company',\n",
              "  'a company that is completely controlled by another company'],\n",
              " ['<pad>  Under what type of rule did tribal chiefs change?</s>',\n",
              "  'colonial',\n",
              "  'Alexandrian',\n",
              "  'Coaster',\n",
              "  'Dalesman',\n",
              "  'a resident of a colony'],\n",
              " ['<pad>  In what year did the British establish indirect company rule in Afghanistan?</s>',\n",
              "  '1842',\n",
              "  'Word not found in Wordnet. So unable to extract distractors.',\n",
              "  None],\n",
              " ['<pad>  What are tipu kings glorified through folklore?</s>',\n",
              "  'powers',\n",
              "  'Ability',\n",
              "  'Attitude',\n",
              "  'Cognitive Factor',\n",
              "  'possession of the qualities (especially mental qualities) required to do something or get something done']]"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    }
  ]
}